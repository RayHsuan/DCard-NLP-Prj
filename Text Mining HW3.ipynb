{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = open('C:/Data/PA-2/stopword_chinese.txt','r',encoding = 'utf-8')\n",
    "stopword = list(tmp.read().split('\\n'))\n",
    "tmp.close()\n",
    "tmp = open('C:/Data/PA-2/punctuation.txt','r',encoding = 'utf-8') #.read().split('\\n')\n",
    "punctuation = list(tmp.read().split('\\n'))\n",
    "tmp.close()\n",
    "#print(punctuation)\n",
    "for i, val in enumerate(punctuation):\n",
    "    if u'\\ufeff' in val:\n",
    "        t = val.split('.')\n",
    "        punctuation[i] = '.'\n",
    "\n",
    "#print(punctuation)    \n",
    "RemovedWords = stopword + punctuation\n",
    "#print(RemovedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "tmp = open('C:/Data/PA-3/TrainingData.txt', 'r',encoding = 'utf-8')\n",
    "docs = list(tmp.read().split('\\n'))\n",
    "tmp.close()\n",
    "lstTrainTag = []\n",
    "lstTrainDocs = []\n",
    "lstTrainTokens = []\n",
    "TrainTokens = Counter()\n",
    "for i, line in enumerate(docs):\n",
    "    #Get topic tag\n",
    "    lstTrainTag.append(line.split('\\t')[1].strip()) #依照文檔讀取每一列的文章斷詞\n",
    "    #Get content\n",
    "    content = \"\"\n",
    "    for tokens in line.split('\\t')[2].strip().split():\n",
    "        if tokens.strip().lower() not in RemovedWords:\n",
    "            content += tokens.strip().lower() + \" \"\n",
    "            TrainTokens[tokens.strip().lower()] += 1\n",
    "    \n",
    "    lstTrainDocs.append(content.strip()) #Remove之後的新Tokens\n",
    "    \n",
    "lstTrainTokens = list(TrainTokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_TFDocToken(docs):\n",
    "    mx_TF_DocToken = np.zeros([len(docs),len(lstTrainTokens)])\n",
    "    for i, doc in enumerate(docs):\n",
    "        for term in doc.split():\n",
    "            if term in lstTrainTokens:\n",
    "                mx_TF_DocToken[i,lstTrainTokens.index(term)] = doc.split().count(term)\n",
    "\n",
    "    return mx_TF_DocToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mx_DocToken = get_TFDocToken(lstTrainDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx_DocToken[10][lstTrainTokens.index(\"綠衫軍\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = mx_DocToken[0]+mx_DocToken[1]+mx_DocToken[2]+mx_DocToken[3]+mx_DocToken[4]+mx_DocToken[5]+mx_DocToken[6]+mx_DocToken[7]+mx_DocToken[8]+mx_DocToken[9]+mx_DocToken[10]+mx_DocToken[11]\n",
    "#len(PoliticsVector)\n",
    "test[lstTrainTokens.index(\"綠衫軍\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numTrainDocs = len(mx_DocToken)  #總文章數.  \n",
    "numWords = len(mx_DocToken[1])   #總Token數\n",
    "percentSports = lstTrainTag.count(\"sports\") / float(len(lstTrainDocs)) #Sports類文章所佔量\n",
    "Politics_v_Num = np.ones(len(mx_DocToken[1])) #avoid zero probability\n",
    "Sports_v_Num = np.ones(len(mx_DocToken[1])) \n",
    "PoliticsTokens = len(lstTrainTokens) #all tokens Vector size 初始值給vector size\n",
    "SportsTokens = len(lstTrainTokens) #all tookens Vector size                  \n",
    "for i, docs in enumerate(lstTrainDocs):\n",
    "    if lstTrainTag[i] == \"sports\":  \n",
    "        Sports_v_Num += mx_DocToken[i]  #在Spoprts類文章的所有Token的出現次數\n",
    "        SportsTokens += sum(mx_DocToken[i]) #所有Sports文章的總字數\n",
    "    else:  \n",
    "        Politics_v_Num += mx_DocToken[i]\n",
    "        PoliticsTokens += sum(mx_DocToken[i])\n",
    "        \n",
    "SportsVector = np.log(Sports_v_Num / SportsTokens)       # 計算 Sports 中每個 token 出現的 Probability\n",
    "PoliticsVector = np.log(Politics_v_Num / PoliticsTokens)  # 計算 Politics 中每個 token 出現的 Probability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.8514689508920332"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SportsVector[lstTrainTokens.index(\"綠衫軍\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "tmp = open('C:/Data/PA-3/TestData.txt', 'r',encoding = 'utf-8')\n",
    "docs = list(tmp.read().split('\\n'))\n",
    "tmp.close()\n",
    "\n",
    "lstTestDocs = []\n",
    "for i, line in enumerate(docs):\n",
    "    #Get content\n",
    "    content = \"\"\n",
    "    for tokens in line.split('\\t')[1].strip().split():\n",
    "        if tokens.strip().lower() not in RemovedWords:\n",
    "            content += tokens.strip().lower() + \" \"\n",
    "    \n",
    "    lstTestDocs.append(content.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mx_testDocToken = get_TFDocToken(lstTestDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MultinomialClassify(docID,testVector,SVector,PVector,pSports):\n",
    "    lstCompare = [] \n",
    "    lstCompare.append(docID)\n",
    "    Sports = sum(testVector * SVector) + np.log(pSports)  \n",
    "    Politics = sum(testVector * PVector) + np.log(1 - pSports)\n",
    "    lstCompare.append(Sports)\n",
    "    lstCompare.append(Politics)\n",
    "    #print(lstCompare)\n",
    "    if Sports > Politics:\n",
    "        lstCompare.append(\"Sports\")\n",
    "    else:\n",
    "        lstCompare.append(\"Politics\")\n",
    "        \n",
    "    return lstCompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#print(len(mx_testDocToken))\n",
    "import pandas as pd\n",
    "lblColumn = [\"docID\",\"Sports_P\",\"Politics_P\",\"Classified_Tag\"]\n",
    "dfResult = pd.DataFrame(columns = lblColumn)\n",
    "\n",
    "for i in range(len(mx_testDocToken)):\n",
    "    result=MultinomialClassify(str(i+3501),mx_testDocToken[i],SportsVector,PoliticsVector,percentSports)\n",
    "    #print(result)\n",
    "    dfResult.loc[dfResult.shape[0]] = result\n",
    "    #print(str(i+3501) + \";\" + result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfResult.to_csv(\"HW3_Result.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
