{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '是', '一', '在', '有', '個', '我', '不', '這', '了', '他', '也', '就', '人', '都', '說', '而', '我們', '你', '要', '之', '會', '對', '及', '和', '與', '以', '很', '種', '中', '大', '能', '著', '她', '那', '上', '但', '年', '還', '可以', '時', '最', '自己', '為', '來', '所', '他們', '兩', '各', '可', '或', '好', '等', '又', '將', '後', '因為', '於', '由', '從', '更', '被', '才', '已', '者', '每', '次', '把', '三', '什麼', '其', '讓', '此', '做', '再', '所以', '只', '沒有', '則', '卻', '地', '並', '位', '得', '想', '去', '呢', '表示', '到', '如果', '为', '于', '后', '来', '因', '下', '这', '与', '并', '个', '无', '小', '们', '起', '今', '亦', '某', '乃', '它', '吧', '比', '别', '趁', '当', '从', '打', '凡', '儿', '尔', '该', '给', '跟', '何', '还', '即', '几', '既', '看', '据', '距', '靠', '啦', '另', '么', '嘛', '拿', '哪', '您', '凭', '且', '却', '让', '仍', '啥', '如', '若', '使', '谁', '虽', '随', '同', '哇', '嗡', '往', '些', '向', '沿', '哟', '用', '咱', '则', '怎', '曾', '至', '致', '着', '诸', '自', '', '[', '.', '!', '/', '-', '>', '<', '$', '%', '^', '*', '(', '\"', '?', ']', '|', '[', '—', '！', '，', '。', '？', '、', '（', '）', '：', '」', '「', ')', '(', '【', '】', '╱', '；', ']', '=', '「', '」', '‧', '（', '）', '.', '~', '']\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import math\n",
    "\n",
    "tmp = open('C:/Data/PA-2/stopword_chinese.txt','r',encoding = 'utf-8')\n",
    "stopword = list(tmp.read().split('\\n'))\n",
    "tmp.close()\n",
    "tmp = open('C:/Data/PA-2/punctuation.txt','r',encoding = 'utf-8') #.read().split('\\n')\n",
    "punctuation = list(tmp.read().split('\\n'))\n",
    "tmp.close()\n",
    "#print(punctuation)\n",
    "for i, val in enumerate(punctuation):\n",
    "    if u'\\ufeff' in val:\n",
    "        t = val.split('.')\n",
    "        punctuation[i] = '.'\n",
    "\n",
    "#print(punctuation)    \n",
    "RemovedWords = stopword + punctuation\n",
    "print(RemovedWords)\n",
    "#print(RemovedWords)\n",
    "#\\s         | 空格、定位符號或換行符號(即比對空白字元)\n",
    "\n",
    "#\\S         | 除了空格、定位符號和換行符號以外的任何字元\n",
    "#movieRegex = re.compile(r'[^funy]') #比對是否有f、u、n、y以外的字元\n",
    "#movieRegex.findall('See you tomorrow.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35532\n",
      "2000\n",
      "2000\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp = open('C:/Data/PA-2/TMBD_news_files_2000.txt', 'r',encoding = 'utf-8')\n",
    "docs = list(tmp.read().split('\\n'))\n",
    "tmp.close()\n",
    "lstTokens = []\n",
    "lstDoc = []\n",
    "dictDoc = {}\n",
    "\n",
    "for i, line in enumerate(docs):\n",
    "    #Get Doc ID\n",
    "    ID = int(line.split('\\t')[0]) #依照文檔讀取每一列Doc ID\n",
    "    #Get Split Terms\n",
    "    content = line.split('\\t')[1].strip() #依照文檔讀取每一列的文章斷詞\n",
    "    content2 = \"\"\n",
    "    #print(ID)\n",
    "    #print(content)\n",
    "    \n",
    "    for tokens in content.split():\n",
    "        if tokens.strip().lower() not in RemovedWords:\n",
    "            if tokens.strip().lower() not in lstTokens:\n",
    "                lstTokens.append(tokens.strip().lower())\n",
    "                \n",
    "            content2 += tokens.strip().lower() + \" \"\n",
    "    #print(content2)\n",
    "    #將 ID 和 所有斷詞記錄在 Dict中\n",
    "    dictDoc[ID] = content2\n",
    "    lstDoc.append(content2)\n",
    "\n",
    "    #if ID == 1961:\n",
    "    #    print(content.split())\n",
    "    #if ID == 2000:\n",
    "        #print(content.split())\n",
    "\n",
    "#print(len(dictDoc))\n",
    "#print(dictDoc[1])\n",
    "#print(dictDoc[2000])\n",
    "print(len(lstTokens))\n",
    "print(len(dictDoc))\n",
    "print(len(lstDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nba 愛 恨 隨便 嘴 米勒 進 名人堂 記者 吳政紘 綜合 報導 記得 miller time 嗎 1995年 nba 季後賽 東區 四 強 賽 溜馬隊 紐約 尼克隊 第1 戰 最後 8.9 秒 8 分絕殺 跳投 就是 reggie miller 代表作 嘖嘖稱奇 比賽 現在 嘴 miller 正式 選入 nba 籃球 名人堂 成為 真正 傳奇 談到 令 尼克 球迷 心碎 逆轉秀 嘴 回憶 道 當 球賽 開始 前 對手 握手 互相 示好 運動家 風度 但是 當 john starks 致意 理會 那時 心想 羞辱 無禮 小子 真的 做到 第4 節 剩 18.7 秒 溜馬隊 105 : 99 落後 6 分 情況 最後 8.9 秒連灌 8 分 最後 帶領 球隊 107 : 105 漂亮 逆轉 勝 地主 難堪 溜馬 球迷 難以 忘懷 儘管 日後 許多 尼克迷 或是 場 外 叫囂 大家 模糊 焦點 不過 別 忘 歷史 定位 miller nba 史上 第2 投進 最多 三分球 出手 球員 史上 第6 最多場 出賽 球員 重要 溜馬隊史 重要 球員 之一 90年代 溜馬隊 光榮 時代 史上 得分 第17 多 偉大 射手 nba 歷史 或是 未來 具有 不可 磨滅 意義 \n"
     ]
    }
   ],
   "source": [
    "print(lstDoc[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "def get_tf(doc):\n",
    "    tf_v = np.zeros([len(lstTokens),1])\n",
    "    for term in doc.split():\n",
    "        tf_v[lstTokens.index(term),0] = doc.split().count(term)\n",
    "    return tf_v\n",
    "\n",
    "lstTFbyDoc = []\n",
    "for doc in range(0,len(lstDoc)):\n",
    "    #print(doc)\n",
    "    #print(lstDoc[doc])\n",
    "    #if doc == 3:\n",
    "    #    break\n",
    "    tf = get_tf(lstDoc[doc])\n",
    "    lstTFbyDoc.append(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.]\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "#ind = lstTokens.index(\"逆轉秀\")\n",
    "#lstTFbyDoc[55][ind]\n",
    "ind = lstTokens.index(\"爸爸\")\n",
    "print(lstTFbyDoc[0][ind])\n",
    "print(len(lstTFbyDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "22\n",
      "35532\n",
      "Wall time: 240 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import OrderedDict\n",
    "DF_byTerm = {}\n",
    "\n",
    "for doc in lstDoc:\n",
    "    terms = list(OrderedDict.fromkeys(doc.split())) # Remove duplicate terms in the same doc\n",
    "    for word in terms:\n",
    "        if word in DF_byTerm:\n",
    "            DF_byTerm[word] = DF_byTerm[word] + 1\n",
    "        else:\n",
    "            DF_byTerm[word] = 1\n",
    "\n",
    "print(DF_byTerm[\"出海\"])\n",
    "print(DF_byTerm['wade'])\n",
    "print(len(DF_byTerm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import numpy as np\n",
    "import math\n",
    "df_v = np.zeros([len(lstTokens),1])\n",
    "idf_v = np.zeros([len(lstTokens),1])\n",
    "\n",
    "lstDF = []\n",
    "\n",
    "for key in DF_byTerm:\n",
    "    #print(key)\n",
    "    #break\n",
    "    df_v[lstTokens.index(key),0] = DF_byTerm[key]\n",
    "    idf_v[lstTokens.index(key),0] = math.log(len(lstDoc)/DF_byTerm[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       ..., \n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#idf_v\n",
    "lstTFbyDoc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 530 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "weight_v = np.zeros([len(lstTokens),1])\n",
    "\n",
    "tfidf_mx = []\n",
    "for doc in range(0, len(lstTFbyDoc)):\n",
    "    weight_v = lstTFbyDoc[doc] * idf_v\n",
    "    tfidf_mx.append(weight_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.40367788]\n",
      "[ 36.07888005]\n"
     ]
    }
   ],
   "source": [
    "#119\n",
    "#出海\n",
    "#wade\n",
    "ind = lstTokens.index(\"出海\")\n",
    "print(tfidf_mx[513][ind])\n",
    "ind = lstTokens.index(\"wade\")\n",
    "print(tfidf_mx[0][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "dictSim = {}\n",
    "def getSimilarity(ref_v,compare_v):\n",
    "    return (np.dot(ref_v.T,compare_v)[0][0]) / (LA.norm(ref_v) * LA.norm(compare_v))\n",
    "\n",
    "j = 0\n",
    "for i, vec in enumerate(tfidf_mx):\n",
    "    j+=1\n",
    "    dictSim[j] = getSimilarity(tfidf_mx[55],tfidf_mx[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56:1.0\n",
      "52:0.18591535180295754\n",
      "1212:0.13120320989537246\n",
      "1149:0.12163022413891184\n",
      "1286:0.10971954023877442\n",
      "1457:0.09415418189441585\n",
      "1209:0.09254373811728546\n",
      "441:0.09056419473116052\n",
      "412:0.08743499068148575\n",
      "1275:0.08717762433055498\n",
      "1451:0.08626882676982027\n",
      "1024:0.08295265947710738\n",
      "1185:0.08193689925532482\n",
      "1164:0.07733825067148763\n",
      "1199:0.07547534371635672\n",
      "1482:0.07455482182689062\n",
      "451:0.07409817587628038\n",
      "1200:0.07228482151223826\n",
      "1479:0.07177648945519635\n",
      "1349:0.07086245359581525\n",
      "1007:0.07083262271889629\n",
      "1473:0.0707952346742143\n",
      "1439:0.07073861058820645\n"
     ]
    }
   ],
   "source": [
    "Sort_Similarity = sorted(dictSim, key = lambda x : dictSim[x], reverse = True)\n",
    "for key in Sort_Similarity:\n",
    "    if dictSim[key] > 0.07:\n",
    "        print(\"{}:{}\".format(key, dictSim[key]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
